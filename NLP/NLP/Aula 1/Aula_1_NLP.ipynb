{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização e N-grama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu gosto de assistir jogos de futebol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já eu, prefiro assistir jogos de basquete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text\n",
       "0      Eu gosto de assistir jogos de futebol\n",
       "1  Já eu, prefiro assistir jogos de basquete"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "      'Eu gosto de assistir jogos de futebol',\n",
    "      'Já eu, prefiro assistir jogos de basquete'\n",
    "    ]\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1\n",
      "assistir  1  1\n",
      "basquete  0  1\n",
      "de        2  1\n",
      "eu        1  1\n",
      "futebol   1  0\n",
      "gosto     1  0\n",
      "jogos     1  1\n",
      "já        0  1\n",
      "prefiro   0  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0  1\n",
      "assistir jogos    1  1\n",
      "de assistir       1  0\n",
      "de basquete       0  1\n",
      "de futebol        1  0\n",
      "eu gosto          1  0\n",
      "eu prefiro        0  1\n",
      "gosto de          1  0\n",
      "jogos de          1  1\n",
      "já eu             0  1\n",
      "prefiro assistir  0  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0  1\n",
      "assistir jogos de       1  1\n",
      "de assistir jogos       1  0\n",
      "eu gosto de             1  0\n",
      "eu prefiro assistir     0  1\n",
      "gosto de assistir       1  0\n",
      "jogos de basquete       0  1\n",
      "jogos de futebol        1  0\n",
      "já eu prefiro           0  1\n",
      "prefiro assistir jogos  0  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'futebol',\n",
       " 'brasileiro',\n",
       " 'é',\n",
       " 'o',\n",
       " 'melhor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " '.',\n",
       " 'Você',\n",
       " 'concorda',\n",
       " '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "exemplo = 'O futebol brasileiro é o melhor do mundo. Você concorda?'\n",
    "words = word_tokenize(exemplo)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen, Aerosmith & Beatles -> ['Queen', 'Aerosmith', 'Beatles']\n",
      "Phone Num :  2004-959-559 \n",
      "Phone Num :  2004959559\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "rex = re.compile('\\w+') #qualquer caracter alfanumérico - compilado\n",
    "bandas = 'Queen, Aerosmith & Beatles'\n",
    "print (bandas, '->', rex.findall(bandas))\n",
    "phone = \"2004-959-559 # This is Phone Number\"\n",
    "num = re.sub('#.*$', \"\", phone) #elimina tudo após #\n",
    "print (\"Phone Num : \", num)\n",
    "num = re.sub(r'\\D', \"\", phone)# só deixa número\n",
    "print (\"Phone Num : \", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gmail']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r\"(?<=@)[^.]+(?=\\.)\"\n",
    "re.findall(regex, \"batman@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gmail'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"batman@gmail.com\"\n",
    "s.split(\"@\")[1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'carro', 'que', 'estava', 'quebrado', 'voltou', 'a', 'funcionar']\n",
      "['Meu', 'carro', 'quebrou', 'e', 'não', 'está', 'funcionando']\n"
     ]
    }
   ],
   "source": [
    "ex1 = 'O carro que estava quebrado voltou a funcionar'\n",
    "ex2 = 'Meu carro quebrou e não está funcionando'\n",
    "\n",
    "print(word_tokenize(ex1))\n",
    "print(word_tokenize(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "estava       1  0\n",
      "está         0  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "meu          0  1\n",
      "não          0  1\n",
      "que          1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text':[ex1,ex2]})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante** notar que, intrinsicamente, CountVectorizer já realiza o processo de passar as palavras para minúscula. Acesse a documentação e veja [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "examples = [\n",
    "   \"go\",\"going\",\n",
    "   \"goes\",\"gone\",\"went\"\n",
    "]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in examples:\n",
    "  print(wnl.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.8.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting pt-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.0 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from pt-core-news-sm==3.4.0) (3.4.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.8.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quebrar', 'quebrar', 'quebrar', 'quebrariam']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(u'quebrou,quebraram,quebrado,quebrariam')\n",
    "[token.lemma_ for token in doc if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\n",
    "    \"connection\",\"connections\",\n",
    "    \"connective\",\"connecting\",\"connected\"\n",
    "]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conecta\n",
      "conectado\n",
      "conectamo\n",
      "desconectado\n",
      "conectividad\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conect\n",
      "conect\n",
      "conect\n",
      "desconect\n",
      "conect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(rslp.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1 = \" \".join([rslp.stem(x) for x in word_tokenize(ex1)])\n",
    "stem2 = \" \".join([rslp.stem(x) for x in word_tokenize(ex2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  1\n",
      "carr     1  1\n",
      "est      1  1\n",
      "funcion  1  1\n",
      "quebr    1  1\n",
      "volt     1  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':[stem1,stem2]})\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n"
     ]
    }
   ],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Um', '>N+art'), ('revivalismo', 'H+n'), ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "floresta.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('um', 'art'),\n",
       " ('revivalismo', 'n'),\n",
       " ('refrescante', 'adj'),\n",
       " ('o', 'art'),\n",
       " ('7_e_meio', 'prop'),\n",
       " ('é', 'v-fin'),\n",
       " ('um', 'art'),\n",
       " ('ex-libris', 'n'),\n",
       " ('de', 'prp'),\n",
       " ('a', 'art')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplify_tag(t):\n",
    "  if \"+\" in t:\n",
    "    return t.split(\"+\")[1]\n",
    "  return t \n",
    "\n",
    "twords = nltk.corpus.floresta.tagged_words()\n",
    "twords = [(w.lower(),simplify_tag(t)) for (w,t) in twords]\n",
    "twords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese Treebank\n",
      "\n",
      "Projecto Floresta Sinta(c)tica -- http://www.linguateca.pt/Floresta/\n",
      "Version 7.4  Distributed with permission.\n",
      "\n",
      "Penn Treebank format, available from http://linguateca.di.uminho.pt/FS/fs.html\n",
      "\n",
      "Key to tags (http://visl.sdu.dk/visl/pt/portsymbol.html)\n",
      "\n",
      "<ACC          direct object\n",
      "<ACC-PASS     passive use of pronoun 'se'\n",
      "<ADVS, <ADVO  adverbial argument\n",
      "<ADVL         adjunct adverbial\n",
      "<DAT          dative (indirect) object\n",
      "<FOC          focus marker (or right focus bracket)\n",
      "<OC           object complement\n",
      "<PASS         agent of passive\n",
      "<PIV          prepositional object\n",
      "<PRED         free (subject) predicative, right of main verb\n",
      "<SC           subject complement\n",
      "<SUBJ         subject\n",
      ">A            adverbial pre-adject (intensifier before adjective, adverb, pronoun or participle)\n",
      ">N            prenominal modifier\n",
      ">P            modifier of prepositional phrase (intensifier, operator or focus adverb)\n",
      ">S            modifier of clause (intensifier, operator or focus adverb)\n",
      "A<            adverbial post-adject (modifier or argument of adjective, adverb or participle)\n",
      "A<ADV         adverbial argument of attributive participle\n",
      "A<ADVL        adverbial adjunct of attributive participle\n",
      "A<PASS        agent of passive after attributive participle\n",
      "A<PIV         prepositional object of attributive participle\n",
      "A<SC          subject complement of attributive participle\n",
      "ACC>          accusative (direct) object\n",
      "ACC>-PASS     passive use of pronoun 'se'\n",
      "ACC>>         double-fronted accusative (direct) object before matrix verb\n",
      "ADVS>, ADVO>  adverbial argument\n",
      "ADVL          top node adverbial\n",
      "ADVL>         adjunct adverbial\n",
      "ADVL>A        adjunct adverbial before attributive participle\n",
      "ADVL>AS<      adjunct adverbial in averbal clause\n",
      "APP           identifying apposition\n",
      "AS<           clause body of averbal clause\n",
      "CO            co-ordinator\n",
      "COM           comparator (heading averbal clause)\n",
      "DAT>          dative (intransitive) object\n",
      "FAUX          finite auxiliary\n",
      "FMV           finite main verb\n",
      "FOC>          focus marker (or left focus bracket)\n",
      "IAUX          non-finite auxiliary\n",
      "IMV           non-finite main verb\n",
      "KOMP<         argument of comparative hook\n",
      "N<            postnominal modifier or argument\n",
      "N<PRED        postnominal (in-group) predicative (or non-identifying apposition)\n",
      "NPHR          top node noun phrase\n",
      "NUM<          second part of numeral chain\n",
      "OC>           object complement\n",
      "P<            argument of preposition\n",
      "PIV>          prepositional object\n",
      "PRD           predicator (heading averbal clause)\n",
      "PRED>         free (subject) predicative, left of main verb\n",
      "PREF          prefix (category being phased out)\n",
      "PRT-AUX<      auxiliary particle\n",
      "S<            statement predicative (sentence apposition)\n",
      "SC>           subject complement\n",
      "SUB           subordinator\n",
      "SUBJ>         subject\n",
      "SUBJ>>        double-fronted subject, with interfering matrix og quoting verb\n",
      "TOP           topic constituent\n",
      "VOK           vocative constituent\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.floresta.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for (word, tag) in twords]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Esse', 'n'),\n",
       " ('é', 'n'),\n",
       " ('um', 'n'),\n",
       " ('exemplo', 'n'),\n",
       " ('utilizando', 'n'),\n",
       " ('o', 'n'),\n",
       " ('marcador', 'n'),\n",
       " ('padrão', 'n')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'Esse é um exemplo utilizando o marcador padrão'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('n')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('o', 'art'),\n",
       "  ('7_e_meio', 'prop'),\n",
       "  ('é', 'v-fin'),\n",
       "  ('um', 'art'),\n",
       "  ('ex-libris', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('a', 'art'),\n",
       "  ('noite', 'n'),\n",
       "  ('algarvia', 'adj'),\n",
       "  ('.', '.')],\n",
       " [('é', 'v-fin'),\n",
       "  ('uma', 'num'),\n",
       "  ('de', 'prp'),\n",
       "  ('as', 'art'),\n",
       "  ('mais', 'adv'),\n",
       "  ('antigas', 'adj'),\n",
       "  ('discotecas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('o', 'art'),\n",
       "  ('algarve', 'prop'),\n",
       "  (',', ','),\n",
       "  ('situada', 'v-pcp'),\n",
       "  ('em', 'prp'),\n",
       "  ('albufeira', 'prop'),\n",
       "  (',', ','),\n",
       "  ('que', 'pron-indp'),\n",
       "  ('continua', 'v-fin'),\n",
       "  ('a', 'prp'),\n",
       "  ('manter', 'v-inf'),\n",
       "  ('os', 'art'),\n",
       "  ('traços', 'n'),\n",
       "  ('decorativos', 'adj'),\n",
       "  ('e', 'conj-c'),\n",
       "  ('as', 'art'),\n",
       "  ('clientelas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('sempre', 'adv'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
    "train = tsents[1000:]\n",
    "test = tsents[:1000]\n",
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('o', 'art'),\n",
       "  ('7_e_meio', 'prop'),\n",
       "  ('é', 'v-fin'),\n",
       "  ('um', 'art'),\n",
       "  ('ex-libris', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('a', 'art'),\n",
       "  ('noite', 'n'),\n",
       "  ('algarvia', 'adj'),\n",
       "  ('.', '.')],\n",
       " [('é', 'v-fin'),\n",
       "  ('uma', 'num'),\n",
       "  ('de', 'prp'),\n",
       "  ('as', 'art'),\n",
       "  ('mais', 'adv'),\n",
       "  ('antigas', 'adj'),\n",
       "  ('discotecas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('o', 'art'),\n",
       "  ('algarve', 'prop'),\n",
       "  (',', ','),\n",
       "  ('situada', 'v-pcp'),\n",
       "  ('em', 'prp'),\n",
       "  ('albufeira', 'prop'),\n",
       "  (',', ','),\n",
       "  ('que', 'pron-indp'),\n",
       "  ('continua', 'v-fin'),\n",
       "  ('a', 'prp'),\n",
       "  ('manter', 'v-inf'),\n",
       "  ('os', 'art'),\n",
       "  ('traços', 'n'),\n",
       "  ('decorativos', 'adj'),\n",
       "  ('e', 'conj-c'),\n",
       "  ('as', 'art'),\n",
       "  ('clientelas', 'n'),\n",
       "  ('de', 'prp'),\n",
       "  ('sempre', 'adv'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17800040072129833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/76rtgv650dq8578qqnhrpvym0000gn/T/ipykernel_15270/1235642322.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tagger0.evaluate(test))\n"
     ]
    }
   ],
   "source": [
    "tagger0 = nltk.DefaultTagger('n')\n",
    "print(tagger0.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522139851733119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/76rtgv650dq8578qqnhrpvym0000gn/T/ipykernel_15270/1679567981.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tagger1.evaluate(test))\n"
     ]
    }
   ],
   "source": [
    "tagger1 = nltk.UnigramTagger(train)\n",
    "print(tagger1.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14626327389300742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/76rtgv650dq8578qqnhrpvym0000gn/T/ipykernel_15270/893570631.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tagger2.evaluate(test))\n"
     ]
    }
   ],
   "source": [
    "tagger2 = nltk.BigramTagger(train)\n",
    "print(tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinação de Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/76rtgv650dq8578qqnhrpvym0000gn/T/ipykernel_15270/2996003665.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('tagger1: ',tagger1.evaluate(test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger1:  0.8740532959326788\n",
      "tagger2:  0.8900420757363254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/76rtgv650dq8578qqnhrpvym0000gn/T/ipykernel_15270/2996003665.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('tagger2: ',tagger2.evaluate(test))\n"
     ]
    }
   ],
   "source": [
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "print('tagger1: ',tagger1.evaluate(test))\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "print('tagger2: ',tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle\n",
    "from pickle import dump\n",
    "output = open('tagger.pkl', 'wb')\n",
    "dump(tagger2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "input = open('tagger.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1:  [('Isso', 'n'), ('é', 'v-fin'), ('para', 'prp'), ('você.', 'n')]\n",
      "text2:  [('para', 'prp'), ('com', 'prp'), ('isso', 'pron-indp')]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Isso é para você.\"\n",
    "text2 = \"para com isso\"\n",
    "tokens1 = text1.split()\n",
    "tokens2 = text2.split()\n",
    "print('text1: ',tagger.tag(tokens1))\n",
    "print('text2: ',tagger.tag(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/dhenyfernandes/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='neg', p_pos=0.3205526272161921, p_neg=0.6794473727838077)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "opinion = TextBlob(\"This movie was horrible!\", analyzer=NaiveBayesAnalyzer())\n",
    "opinion.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ayrton', 'Senna', 'foi', 'o', 'melhor', 'piloto', 'de', 'Fórmula', '1', 'que', 'já', 'existiu']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(u'Ayrton Senna foi o melhor piloto de Fórmula 1 que já existiu')\n",
    "print([token.orth_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ayrton', 'PROPN'),\n",
       " ('Senna', 'PROPN'),\n",
       " ('foi', 'AUX'),\n",
       " ('o', 'DET'),\n",
       " ('melhor', 'ADJ'),\n",
       " ('piloto', 'NOUN'),\n",
       " ('de', 'ADP'),\n",
       " ('Fórmula', 'PROPN'),\n",
       " ('1', 'PROPN'),\n",
       " ('que', 'PRON'),\n",
       " ('já', 'ADV'),\n",
       " ('existiu', 'VERB')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.orth_, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtrando apenas verbos: \n",
      "['existir']\n",
      "identificação de entidades: \n",
      "(Machado de Assis, Brasil, Academia Brasileira de Letras)\n"
     ]
    }
   ],
   "source": [
    "print('filtrando apenas verbos: ')\n",
    "print([token.lemma_ for token in doc if token.pos_ == 'VERB'])\n",
    "print('identificação de entidades: ')\n",
    "doc1 = nlp(u'Machado de Assis um dos melhores escritores do Brasil, \\\n",
    "foi o primeiro presidente da Academia Brasileira de Letras')\n",
    "print(doc1.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
