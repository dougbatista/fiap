{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\logonpflocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\logonpflocal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('uci-news-aggregator.csv')\n",
    "df = df[['TITLE','CATEGORY']]\n",
    "#categories: b = business, t = science and technology, e = entertainment, m = health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Agents of S.H.I.E.L.D.' Season Finale Sneak P...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mosquito-borne virus may soon hit GA, health d...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chiquita combines with Dublin-based Fyffes to ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Memorial Day weekend and meteor shower viewing...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meet the Visionary Behind the World's Largest IPO</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  'Agents of S.H.I.E.L.D.' Season Finale Sneak P...        e\n",
       "1  Mosquito-borne virus may soon hit GA, health d...        m\n",
       "2  Chiquita combines with Dublin-based Fyffes to ...        b\n",
       "3  Memorial Day weekend and meteor shower viewing...        t\n",
       "4  Meet the Visionary Behind the World's Largest IPO        b"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle - embaralha as linhas para evitar problema (aprende apenas amostras de uma categoria e isso pode levar o gradiente a\n",
    "# ficar preso num mínimo local e só aprender bem sobre)\n",
    "df = shuffle(df)\n",
    "df = df.reset_index(drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "    \n",
    "        \n",
    "def normalize_accents(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def normalize_str(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = normalize_accents(text)\n",
    "    text = re.sub(re.compile(r\" +\"), \" \",text)\n",
    "    return \" \".join([w for w in text.split()])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    table = str.maketrans({key: \" \" for key in punctuations})\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\") # portuguese, caso o dataset seja em português\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)\n",
    "        text = \"\".join([w for w in text if not w.isdigit()])\n",
    "        text = word_tokenize(text)\n",
    "        text = [x for x in text if x not in stop_words]\n",
    "        text = [y for y in text if len(y) > 2]\n",
    "        return [t for t in text]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title_Treated'] = df['TITLE'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>Title_Treated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Agents of S.H.I.E.L.D.' Season Finale Sneak P...</td>\n",
       "      <td>e</td>\n",
       "      <td>[agents, season, finale, sneak, peek, coulson,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mosquito-borne virus may soon hit GA, health d...</td>\n",
       "      <td>m</td>\n",
       "      <td>[mosquito, borne, virus, may, soon, hit, healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chiquita combines with Dublin-based Fyffes to ...</td>\n",
       "      <td>b</td>\n",
       "      <td>[chiquita, combines, dublin, based, fyffes, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Memorial Day weekend and meteor shower viewing...</td>\n",
       "      <td>t</td>\n",
       "      <td>[memorial, day, weekend, meteor, shower, viewi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meet the Visionary Behind the World's Largest IPO</td>\n",
       "      <td>b</td>\n",
       "      <td>[meet, visionary, behind, world, largest, ipo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY  \\\n",
       "0  'Agents of S.H.I.E.L.D.' Season Finale Sneak P...        e   \n",
       "1  Mosquito-borne virus may soon hit GA, health d...        m   \n",
       "2  Chiquita combines with Dublin-based Fyffes to ...        b   \n",
       "3  Memorial Day weekend and meteor shower viewing...        t   \n",
       "4  Meet the Visionary Behind the World's Largest IPO        b   \n",
       "\n",
       "                                       Title_Treated  \n",
       "0  [agents, season, finale, sneak, peek, coulson,...  \n",
       "1  [mosquito, borne, virus, may, soon, hit, healt...  \n",
       "2  [chiquita, combines, dublin, based, fyffes, cr...  \n",
       "3  [memorial, day, weekend, meteor, shower, viewi...  \n",
       "4     [meet, visionary, behind, world, largest, ipo]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(df['CATEGORY']) # label para cada uma das frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parâmetros do word2vec\n",
    "dim_vec = 300\n",
    "min_count = 10\n",
    "window = 4\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instância do Word2Vec\n",
    "modelo = Word2Vec(df[\"Title_Treated\"],\n",
    "                    min_count = min_count, \n",
    "                    vector_size = dim_vec, \n",
    "                    window = window,\n",
    "                    seed = seed,\n",
    "                    workers = num_workers,\n",
    "                    sg = 1) #sg = 0 -> CBOW e sg = 1 -> skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário do Word2Vec:  16241\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do vocabulário do Word2Vec: \", len(modelo.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('galaxy', 0.6372635960578918), ('tab', 0.5986858010292053), ('neo', 0.5947222709655762), ('waterproof', 0.583037793636322), ('specification', 0.5822816491127014), ('antutu', 0.5815654397010803), ('electronics', 0.5799608826637268), ('tizen', 0.5776135325431824), ('phablets', 0.5722646117210388), ('spire', 0.5720193982124329)] \n",
      "\n",
      "0.29598713 \n",
      "\n",
      "[('film', 0.44674697518348694), ('gritty', 0.3722717761993408), ('cinematic', 0.368047833442688)]\n"
     ]
    }
   ],
   "source": [
    "# exemplos das relações semânticas que o word2vec consegue estabelecer\n",
    "print(modelo.wv.most_similar('samsung'), '\\n') # palavra mais similar a 'itau'\n",
    "print(modelo.wv.similarity('google', 'microsoft'), '\\n') # similaridade entre duas palavras\n",
    "print(modelo.wv.most_similar(positive = ['show', 'movie'], negative = ['home'], topn = 3)) # similaridade considerando exemplos positivos e negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanVector(model,phrase):\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    phrase = \" \".join(phrase)\n",
    "    phrase = [x for x in word_tokenize(phrase) if x in vocab]\n",
    "    #Quando não houver palavra o vector recebe 0 para todas as posições\n",
    "    if phrase == []:\n",
    "        vetor = [0.0]*dim_vec \n",
    "    else: \n",
    "        #Caso contrário, calculando a matriz da frase\n",
    "        vetor = np.mean([model.wv[word] for word in phrase],axis=0)\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatures(base): \n",
    "    features = [meanVector(modelo,base['Title_Treated'][i])for i in range(len(base))]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.47298977e-01, -1.29128145e-02,  4.25189920e-02, -3.33455647e-03,\n",
       "       -2.56209701e-01, -5.32733619e-01,  2.47667789e-01,  9.31246504e-02,\n",
       "       -1.74444035e-01, -2.32230142e-01,  3.82345319e-01, -2.64998171e-02,\n",
       "       -1.18765354e-01,  1.47529960e-01,  9.99135002e-02,  8.77503380e-02,\n",
       "       -2.82531500e-01,  4.97776009e-02, -6.84843808e-02, -3.18773687e-01,\n",
       "        6.18683882e-02, -1.63953587e-01,  4.93220925e-01,  1.01788737e-01,\n",
       "        5.13082035e-02,  2.60523826e-01, -2.79241234e-01,  2.35770687e-01,\n",
       "       -7.67095312e-02,  3.23193558e-02, -2.88207293e-01, -7.75894970e-02,\n",
       "        9.78389662e-03, -1.84486672e-01,  1.87153623e-01,  9.61149558e-02,\n",
       "       -2.00837851e-03,  1.65779199e-02, -8.18101689e-02, -7.91890845e-02,\n",
       "       -1.77113846e-01,  3.21817622e-02,  4.30391692e-02,  1.07597478e-01,\n",
       "       -4.76628244e-02, -2.29110911e-01, -1.29013509e-03, -2.72823513e-01,\n",
       "       -5.16905077e-02,  5.75333424e-02,  8.29335079e-02,  3.69382948e-01,\n",
       "       -8.10828879e-02,  2.18864977e-01,  1.83664128e-01,  6.95901141e-02,\n",
       "       -6.19252212e-02,  1.75359681e-01, -1.76970303e-01,  1.71334445e-01,\n",
       "       -2.52918359e-02,  1.49456218e-01, -9.67902318e-02,  1.49182469e-01,\n",
       "        2.12234899e-01, -1.14850082e-01,  1.75569639e-01, -2.19432667e-01,\n",
       "       -5.16593009e-02,  3.09512407e-01, -2.17234001e-01,  1.01518340e-01,\n",
       "       -4.34888154e-02,  1.57018065e-01,  2.12479550e-02, -1.03187561e-01,\n",
       "        1.08490020e-01,  3.05864159e-02, -2.11587176e-01,  3.00384592e-02,\n",
       "       -1.47691473e-01, -2.15639677e-02, -3.25386524e-01,  1.57755241e-01,\n",
       "       -1.27039030e-01,  2.02364624e-01,  3.01715955e-02,  2.18833640e-01,\n",
       "       -1.42528221e-01, -2.80454725e-01,  2.05186345e-02,  2.56606460e-01,\n",
       "        8.90551507e-02, -1.38782905e-02,  1.74323976e-01,  8.71937498e-02,\n",
       "       -2.74624467e-01,  2.28519812e-01, -6.16622306e-02,  1.87584698e-01,\n",
       "        8.85815918e-03,  1.15532184e-03,  4.75632101e-02, -1.11410819e-01,\n",
       "        7.51562640e-02,  2.56075621e-01,  6.77703694e-02, -2.21420407e-01,\n",
       "       -1.62068322e-01, -2.51434594e-01, -2.56125927e-01,  1.02378286e-01,\n",
       "        2.64584035e-01, -2.55460978e-01, -1.81984548e-02,  1.77195687e-02,\n",
       "       -3.99834700e-02, -1.66228995e-01,  1.71477601e-01,  1.18336320e-01,\n",
       "       -1.54885158e-01,  1.71006203e-01, -2.00989559e-01,  2.36351147e-01,\n",
       "       -3.03333700e-02,  2.68399596e-01,  1.58933803e-01,  3.24093342e-01,\n",
       "        2.88472082e-02, -2.54004747e-01, -3.03305119e-01,  7.23096728e-02,\n",
       "        2.50616729e-01, -1.73964664e-01,  1.32072181e-01,  1.43029585e-01,\n",
       "        6.72613308e-02,  1.67271420e-01,  2.82459170e-01,  2.12136105e-01,\n",
       "        1.71022698e-01, -2.00843871e-01, -2.56193876e-01,  1.10598691e-01,\n",
       "        4.64069732e-02, -1.53767809e-01, -7.98754320e-02,  1.24430992e-01,\n",
       "        1.15083583e-01, -2.79054731e-01,  1.46482944e-01,  1.57710630e-02,\n",
       "       -2.85071522e-01,  1.30046427e-01,  1.99081972e-01,  3.75099517e-02,\n",
       "        1.55945450e-01, -2.98548397e-02,  9.51117203e-02,  2.35026077e-04,\n",
       "        7.83234835e-02,  3.97955269e-01, -9.42770615e-02, -1.50476083e-01,\n",
       "       -1.38386935e-02, -2.27492407e-01,  2.32577726e-01,  1.32155672e-01,\n",
       "        2.65765637e-01, -7.67393485e-02,  1.00964643e-01, -3.14425230e-02,\n",
       "       -6.63402155e-02,  1.98819160e-01, -3.07820112e-01, -2.31271103e-01,\n",
       "       -6.33116066e-03, -3.05182040e-01, -2.39010602e-02, -4.53410864e-01,\n",
       "       -3.47832173e-01,  1.91925302e-01, -1.37153372e-01,  2.32848898e-01,\n",
       "        1.62163869e-01, -1.64544702e-01,  4.42450307e-02, -1.19799010e-01,\n",
       "        2.16390565e-01,  1.43629089e-01, -2.10701391e-01,  1.38243482e-01,\n",
       "       -3.90394777e-02,  7.04251230e-03,  1.87425181e-01, -3.60044725e-02,\n",
       "        3.56649943e-02,  9.56946313e-02,  2.25806490e-01,  5.93772344e-02,\n",
       "        2.18957122e-02, -4.95680183e-01,  5.98784350e-02,  1.62310451e-01,\n",
       "       -1.01321124e-01, -2.06324104e-02,  9.91806984e-02,  2.43620202e-01,\n",
       "        5.13137542e-02, -1.45728603e-01, -1.23578630e-01,  8.65856186e-02,\n",
       "       -3.02989751e-01, -2.50593629e-02, -2.34924302e-01, -1.22296689e-02,\n",
       "       -2.60673254e-03,  7.34183714e-02,  6.46412149e-02,  8.29321444e-02,\n",
       "        1.73842147e-01, -2.84754634e-01,  1.24522544e-01,  8.75414684e-02,\n",
       "       -2.55892217e-01, -3.52313034e-02, -8.51314887e-02, -2.43881736e-02,\n",
       "        4.72048521e-02,  2.75021404e-01, -2.34046116e-01,  1.97593033e-01,\n",
       "       -3.27241682e-02,  2.44864225e-02, -2.02126101e-01, -3.64162296e-01,\n",
       "        1.24655724e-01,  2.73292214e-01, -3.03012282e-01,  1.35324776e-01,\n",
       "       -2.69373924e-01, -1.38236172e-02, -3.83439541e-01,  1.58418521e-01,\n",
       "        3.14535528e-01, -7.74724269e-03,  2.54192459e-03,  8.88739154e-02,\n",
       "        1.32904783e-01, -2.39175379e-01, -2.39069715e-01,  3.43831301e-01,\n",
       "        1.94944680e-01, -1.62673727e-01,  1.03144370e-01,  7.05042556e-02,\n",
       "        1.40550986e-01,  1.82352409e-01, -5.88165820e-02, -2.91460127e-01,\n",
       "       -5.71272671e-02, -9.58315507e-02,  1.23793734e-02, -1.16928458e-01,\n",
       "        1.22769631e-01, -1.83493003e-01,  1.77327693e-02, -6.84213862e-02,\n",
       "       -3.19220752e-01,  2.35288993e-01, -9.92305651e-02, -1.81043968e-01,\n",
       "       -2.87476238e-02,  7.46905431e-02, -1.17857479e-01, -7.29134902e-02,\n",
       "        6.71708286e-02, -1.72981367e-01, -8.02767351e-02,  3.23134869e-01,\n",
       "        1.12596147e-01,  1.23929456e-02,  1.15156315e-01,  2.31861323e-02,\n",
       "       -2.06928685e-01, -2.11860895e-01, -2.38704290e-02, -1.49624869e-01,\n",
       "       -6.32585883e-02, -2.72205025e-01,  6.64704069e-02, -1.36556491e-01,\n",
       "       -4.02485058e-02, -4.93969768e-02,  4.46961612e-01, -1.97082296e-01,\n",
       "       -1.55022750e-02, -1.38660640e-01, -1.36631116e-01, -2.72880256e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = createFeatures(df)\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[0:100000], labels[0:100000], test_size=0.3,random_state=42)\n",
    "clf = svm.SVC(kernel='rbf') \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
